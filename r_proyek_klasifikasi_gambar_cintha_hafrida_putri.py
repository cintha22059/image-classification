# -*- coding: utf-8 -*-
"""R_Proyek_Klasifikasi_Gambar_Cintha_Hafrida_Putri.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YadQdsrNKq2IP8lrPVLO8WCsor9DjV9v

Nama : Cintha Hafrida Putri

Email : devdan2510@gmail.com

ID Dicoding : cintha_bang
"""

!pip install tensorflow==2.15.0

# Commented out IPython magic to ensure Python compatibility.
# Mengimpor libraries umum yang sering digunakan
import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq

# Mengimpor libraries untuk visualisasi
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.image import imread

# Mengimpor libraries untuk pemrosesan data gambar
import cv2
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

# Mengimpor libraries untuk pembuatan dan evaluasi model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mencetak versi TensorFlow yang sedang digunakan
print(tf.__version__)

"""## **Data Loading**"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d andrewmvd/animal-faces
!unzip animal-faces.zip

os.listdir('./')

import os

# Tentukan path ke dataset
dataset_path = "./afhq"

# Print struktur direktori
for root, dirs, files in os.walk(dataset_path):
    print(f"Root: {root}")
    for dir_name in dirs:
        print(f"  Directory: {dir_name}")
    for file_name in files:
        print(f"  File: {file_name}")

import os
import shutil

# Tentukan path sumber dan tujuan
source_paths = ["./afhq/train", "./afhq/val"]  # Daftar direktori sumber
class_names = ['cat', 'dog', 'wild']  # Sesuaikan dengan nama kelas yang ada
destination_path = "./animal_faces"

# Membuat direktori tujuan jika belum ada
os.makedirs(destination_path, exist_ok=True)

# Buat subfolder untuk setiap kelas di dalam folder tujuan
for class_name in class_names:
    os.makedirs(os.path.join(destination_path, class_name), exist_ok=True)

# Menampilkan file dalam direktori sumber untuk debugging
for source_path in source_paths:
    print(f"Files in source directory '{source_path}':")
    print(os.listdir(source_path))  # Menampilkan file dalam direktori sumber

# Pindahkan gambar ke dalam subfolder berdasarkan kelas
for source_path in source_paths:
    for image_name in os.listdir(source_path):
        print(f"Checking file: {image_name}")  # Debugging
        try:
            if 'cat' in image_name:
                print(f"Moving {image_name} to cats folder.")
                shutil.move(os.path.join(source_path, image_name), os.path.join(destination_path, 'cat', image_name))
            elif 'dog' in image_name:
                print(f"Moving {image_name} to dogs folder.")
                shutil.move(os.path.join(source_path, image_name), os.path.join(destination_path, 'dog', image_name))
            elif 'wild' in image_name:
                print(f"Moving {image_name} to wild folder.")
                shutil.move(os.path.join(source_path, image_name), os.path.join(destination_path, 'wild', image_name))
        except Exception as e:
            print(f"Failed to move {image_name}: {e}")

print("Images have been sorted into their respective class folders.")

import os
import shutil

# Tentukan daftar folder utama yang akan menjadi tujuan akhir
main_folders = ['./animal_faces/cat', './animal_faces/dog', './animal_faces/wild']

# Loop melalui setiap folder utama
for main_folder in main_folders:
    # Loop melalui semua subfolder yang ada di dalam folder utama
    for root, dirs, files in os.walk(main_folder):
        for file in files:
            # Cek apakah file adalah gambar (misalnya, file .jpg atau .jpeg atau .png)
            if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png'):
                file_path = os.path.join(root, file)  # Dapatkan path lengkap dari file
                new_path = os.path.join(main_folder, file)  # Tentukan path baru di folder utama

                # Pindahkan file ke folder utama
                shutil.move(file_path, new_path)

    # Hapus folder kosong setelah memindahkan semua file
    for root, dirs, files in os.walk(main_folder, topdown=False):
        for dir in dirs:
            os.rmdir(os.path.join(root, dir))

print("Semua file telah dipindahkan ke folder utama masing-masing dan subfolder kosong telah dihapus.")

os.listdir('./')

"""## **Plot Gambar Sample untuk Semua Kelas**"""

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
lung_image = {}

# Tentukan path sumber train
path = "./"
path_sub = os.path.join(path, "animal_faces")

# Mendapatkan daftar direktori yang valid (tidak dimulai dengan '.')
valid_directories = [d for d in os.listdir(path_sub) if not d.startswith('.')]

for i in valid_directories:
    lung_image[i] = os.listdir(os.path.join(path_sub, i))

# Menampilkan secara acak 5 gambar di bawah setiap dari 2 kelas dari data.
# Anda akan melihat gambar yang berbeda setiap kali.
path_sub = "./animal_faces/"

# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih
fig, axs = plt.subplots(len(lung_image.keys()), 5, figsize=(15, 15))

# Iterasi melalui direktori yang valid
for i, class_name in enumerate(valid_directories):
    images = np.random.choice(lung_image[class_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, class_name, image_name)
        img = Image.open(img_path).convert("L")  # Konversi menjadi skala keabuan
        axs[i, j].imshow(img, cmap='gray')
        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

import seaborn as sns
# Define source path
lung_path = "./animal_faces/"

# Create a list that stores data for each filenames, filepaths, and labels in the data
file_name = []
labels = []
full_path = []

# Get data image filenames, filepaths, labels one by one with looping, and store them as dataframe
for path, subdirs, files in os.walk(lung_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})

# Plot the distribution of images across the classes
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

"""## **Data Preprocessing**

## Data Augmentation
"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import random

# Membuat fungsi untuk melakukan rotasi berlawanan arah jarum jam
def anticlockwise_rotation(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.rot90(img, k=random.randint(1, 4))  # Rotasi 90, 180, atau 270 derajat secara acak
    return img

# Membuat fungsi untuk melakukan rotasi searah jarum jam
def clockwise_rotation(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.rot90(img, k=random.randint(1, 4))  # Rotasi 90, 180, atau 270 derajat secara acak
    return img

# Membuat fungsi untuk membalik gambar secara vertikal dari atas ke bawah
def flip_up_down(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.flip_up_down(img)
    return img

# Membuat fungsi untuk memberikan efek peningkatan kecerahan pada gambar
def add_brightness(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.adjust_brightness(img, delta=random.uniform(0.1, 0.5))  # Sesuaikan nilai delta sesuai kebutuhan
    return img

# Membuat fungsi untuk memberikan efek blur pada gambar
def blur_image(img):
    img = tf.image.resize(img, (224, 224))
    img = tf.image.random_blur(img, (3, 3))  # Ukuran kernel blur bisa disesuaikan
    return img

# Membuat fungsi untuk memberikan efek pergeseran acak pada gambar
def sheared(img):
    img = tf.image.resize(img, (224, 224))
    # Buat objek ImageDataGenerator dengan parameter shearing range
    datagen = ImageDataGenerator(shear_range=0.2)
    img = next(iter(datagen.flow(tf.expand_dims(img, 0))))[0]
    return img

# Membuat fungsi untuk melakukan pergeseran melengkung pada gambar
def warp_shift(img):
    img = tf.image.resize(img, (224, 224))
    # Buat objek ImageDataGenerator dengan parameter width_shift_range dan height_shift_range
    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1)
    img = next(iter(datagen.flow(tf.expand_dims(img, 0))))[0]
    return img

import os
import random
from skimage import io
from skimage.util import img_as_ubyte
import cv2

# Fungsi transformasi (anda perlu mendefinisikan fungsi-fungsi ini)
def anticlockwise_rotation(image):
    return cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)

def clockwise_rotation(image):
    return cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)

def warp_shift(image):
    # Implementasikan fungsi untuk melakukan warp shift
    pass

def blur_image(image):
    return cv2.GaussianBlur(image, (5, 5), 0)

def add_brightness(image, value=30):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hsv[:, :, 2] = cv2.add(hsv[:, :, 2], value)
    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

def flip_up_down(image):
    return cv2.flip(image, 0)

def sheared(image):
    # Implementasikan fungsi untuk melakukan shear
    pass

# Kamus transformasi
transformations = {
    'rotate anticlockwise': anticlockwise_rotation,
    'rotate clockwise': clockwise_rotation,
    'warp shift': warp_shift,
    'blurring image': blur_image,
    'add brightness': add_brightness,
    'flip up down': flip_up_down,
    'shear image': sheared
}

# Jalur ke gambar asli (kucing, anjing, hewan liar)
images_paths = ['./animal_faces/cat', './animal_faces/dog', './animal_faces/wild']
augmented_path = "animal_faces_augmented"  # Jalur untuk menyimpan gambar yang diaugmentasi
images = []  # Untuk menyimpan gambar yang telah diproses dari folder

# Membaca nama gambar dari setiap folder dan menambahkan jalur ke dalam array "images"
for path in images_paths:
    for im in os.listdir(path):
        # Memeriksa apakah file tersebut adalah gambar dengan ekstensi yang valid
        if im.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
            images.append(os.path.join(path, im))

# Debug: Tampilkan jumlah gambar yang ditemukan
print(f'Jumlah gambar yang ditemukan: {len(images)}')

# Periksa apakah ada gambar untuk diaugmentasi
if len(images) == 0:
    print('Tidak ada gambar yang ditemukan di folder. Silakan periksa jalur atau ekstensi file.')
else:
    # Jumlah gambar yang akan ditambahkan dengan hasil transformasi augmentasi
    images_to_generate = 2000
    i = 1

    while i <= images_to_generate:
        image = random.choice(images)
        try:
            original_image = io.imread(image)

            # Memeriksa apakah gambar memiliki dimensi yang valid (3 atau 4 dimensi)
            if original_image.ndim not in [3, 4]:
                raise ValueError('Invalid image dimensions')

            transformed_image = None
            n = 0  # Variabel untuk melakukan iterasi sampai jumlah transformasi yang akan diterapkan
            transformation_count = random.randint(1, len(transformations))  # Pilih jumlah transformasi acak

            while n < transformation_count:  # Ubah <= menjadi < untuk menghindari satu transformasi tidak dipakai
                key = random.choice(list(transformations))  # Memilih dan memanggil metode secara acak
                transformed_image = transformations[key](original_image)
                n += 1

            new_image_path = "%s/augmented_image_%s.jpg" % (augmented_path, i)
            transformed_image = img_as_ubyte(transformed_image)  # Mengonversi gambar ke format byte
            cv2.imwrite(new_image_path, transformed_image)  # Simpan hasil transformasi augmentasi
            i += 1
        except ValueError as e:
            print('Could not read or process the image', image, ':', e, 'hence skipping it.')
        except Exception as e:
            print('An error occurred while processing the image', image, ':', e)

# Define source path
lung_path = "./animal_faces/"

# Create a list that stores data for each filenames, filepaths, and labels in the data
file_name = []
labels = []
full_path = []

# Get data image filenames, filepaths, labels one by one with looping, and store them as dataframe
for path, subdirs, files in os.walk(lung_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})

# Plot the distribution of images across the classes
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

"""## Split Dataset Train and Test"""

# Panggil variabel mypath yang menampung folder dataset gambar
mypath= './animal_faces/'

file_name = []
labels = []
full_path = []
for path, subdirs, files in os.walk(mypath):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)


# Memasukan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapih
df = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})
# Melihat jumlah data gambar pada masing-masing label
df.groupby(['labels']).size()

# Variabel yang digunakan pada pemisahan data ini dimana variabel x = data path dan y = data labels
X= df['path']
y= df['labels']

# Split dataset awal menjadi data train dan test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=300)

# Menyatukan kedalam masing-masing dataframe
df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})
df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})

# Print hasil diatas untuk melihat panjang size data training dan testing
print('train size', len(df_tr))
print('test size', len(df_te))

# Gabungkan DataFrame df_tr dan df_te
df_all = pd.concat([df_tr, df_te], ignore_index=True)

print('===================================================== \n')
print(df_all.groupby(['set', 'labels']).size(), '\n')
print('===================================================== \n')

# Cek sample data
print(df_all.sample(5))

# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya
datasource_path = "./animal_faces/"
# Membuat variabel Dataset, dimana nanti menampung data yang telah dilakukan pembagian data training dan testing
dataset_path = "Dataset-Final/"

for index, row in tq(df_all.iterrows()):
    # Deteksi filepath
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['labels'],row['image'].split('.')[0])

    # Buat direktori tujuan folder
    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))

    # Tentukan tujuan file
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)

    # Salin file dari sumber ke tujuan
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

"""## **Image Data Generator**"""

# Define training and test directories
TRAIN_DIR = "Dataset-Final/train/"
TEST_DIR = "Dataset-Final/test/"

train_cat = os.path.join(TRAIN_DIR, 'cat')
train_dog = os.path.join(TRAIN_DIR, 'dog')
train_wild = os.path.join(TRAIN_DIR, 'wild')
test_cat = os.path.join(TEST_DIR, 'cat')
test_dog = os.path.join(TEST_DIR, 'dog')
test_wild = os.path.join(TEST_DIR, 'wild')


print("Total number of cat images in training set: ", len(os.listdir(train_cat)))
print("Total number of dog images in training set: ", len(os.listdir(train_dog)))
print("Total number of wild images in training set: ", len(os.listdir(train_wild)))
print("Total number of cat images in test set: ", len(os.listdir(test_cat)))
print("Total number of dog images in test set: ", len(os.listdir(test_dog)))
print("Total number of wild images in test set: ", len(os.listdir(test_wild)))

# Create an ImageDataGenerator object normalizing the images
datagen = ImageDataGenerator(rescale=1/255)
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = datagen.flow_from_directory(TRAIN_DIR,
                                              batch_size=32,
                                              target_size=(100,100),
                                              color_mode="grayscale",
                                              class_mode='categorical',
                                              subset='training',
                                              shuffle=True)

test_generator = test_datagen.flow_from_directory(TEST_DIR,
                                                  batch_size=1,
                                                  target_size=(100,100),
                                                  color_mode="grayscale",
                                                  class_mode='categorical',
                                                  shuffle=False)

"""## MODEL"""

# tf.keras.backend.clear_session()

####################### Init sequential model ##################################
model_final = Sequential()

# ######################### Input layer with Fully Connected Layer ################################
# 1st Convolutional layer, Batch Normalization layer, and Pooling layer
model_final.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(100,100,1)))
model_final.add(BatchNormalization())
model_final.add(MaxPooling2D((2, 2)))

# 2nd Convolutional layer, Batch Normalization layer, and Pooling layer
model_final.add(Conv2D(32, (4, 4), padding='same', activation='relu'))
model_final.add(BatchNormalization())
model_final.add(MaxPooling2D((2, 2)))

# 3rd Convolutional layer, Batch Normalization layer, and Pooling layer
model_final.add(Conv2D(32, (7, 7), padding='same', activation='relu'))
model_final.add(BatchNormalization())
model_final.add(MaxPooling2D((2, 2)))

# Flatten layer
model_final.add(Flatten())

# 1st Dense Layer
model_final.add(Dense(128, activation='relu'))
# 1st Dropout Layer
model_final.add(Dropout(0.5))

# 2nd Dense Layer
model_final.add(Dense(64, activation='relu'))
# 2nd Dropout Layer
model_final.add(Dropout(0.3))

# 3rd Dense Layer (tambahan)
model_final.add(Dense(32, activation='relu'))
# 3rd Dropout Layer (tambahan)
model_final.add(Dropout(0.2))

# Final Dense layer dengan multiple output units dan softmax activation
model_final.add(Dense(3, activation='softmax'))  # ubah angka 3 sesuai jumlah kelas yang diinginkan

######################### Compile Model ################################
model_final.compile(optimizer=tf.keras.optimizers.RMSprop(),
                   loss='categorical_crossentropy',
                   metrics=['accuracy'])

# Summary of the Model Architecture
print(model_final.summary())

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import os

# Callback untuk menghentikan pelatihan berdasarkan kondisi val_accuracy dan epoch
class createEarlyStopping(tf.keras.callbacks.Callback):
    def __init__(self, patience=7):
        super(createEarlyStopping, self).__init__()
        self.patience = patience
        self.best_val_accuracy = 0
        self.wait = 0

    def on_epoch_end(self, epoch, logs=None):
        # Ensure logs is a dictionary and handle missing 'val_accuracy'
        logs = logs or {}
        current_val_accuracy = logs.get('val_accuracy')

        # Check if val_accuracy is available before comparison
        if current_val_accuracy is not None:
            # Cek jika val_accuracy lebih dari 0.95 dan epoch lebih dari atau sama dengan 10
            if current_val_accuracy >= 0.95 and epoch >= 10:
                print('\n# Callback stopping training at epoch:', epoch,
                      'with val_accuracy: %2.2f%%' % (current_val_accuracy * 100))
                self.model.stop_training = True

            # Menghentikan pelatihan jika tidak ada peningkatan val_accuracy selama `patience` epoch
            elif current_val_accuracy <= self.best_val_accuracy:
                self.wait += 1
                if self.wait >= self.patience:
                    print('\n# Callback stopping training due to no improvement after', self.patience, 'epochs at epoch:', epoch)
                    self.model.stop_training = True
            else:
                # Update val_accuracy terbaik dan reset counter wait
                self.best_val_accuracy = current_val_accuracy
                self.wait = 0

# Hitung jumlah gambar untuk setiap kelas
train_cat = os.path.join("Dataset-Final/train/cat")
train_dog = os.path.join("Dataset-Final/train/dog")
train_wild = os.path.join("Dataset-Final/train/wild")

count_cat = len(os.listdir(train_cat))
count_dog = len(os.listdir(train_dog))
count_wild = len(os.listdir(train_wild))

# Debug: Tampilkan jumlah gambar untuk setiap kelas
print("Jumlah gambar cat: ", count_cat)
print("Jumlah gambar dog: ", count_dog)
print("Jumlah gambar wild: ", count_wild)

# Menghitung bobot untuk setiap kelas
total_images = count_cat + count_dog + count_wild
weight_cat = (1 / count_cat) * (total_images / 3.0)
weight_dog = (1 / count_dog) * (total_images / 3.0)
weight_wild = (1 / count_wild) * (total_images / 3.0)

# Mendefinisikan class_weights untuk tiga kelas
class_weights = {0: weight_cat, 1: weight_dog, 2: weight_wild}

# Memulai pelatihan model dengan callback dan class_weights
# %time
history_final = model_final.fit(
    train_generator,
    epochs=30,
    batch_size=32,
    validation_data=test_generator,
    class_weight=class_weights,
    callbacks=[createEarlyStopping()]
)

acc = history_final.history['accuracy']
val_acc = history_final.history['val_accuracy']
loss = history_final.history['loss']
val_loss = history_final.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.title('Training and Validaion Loss')
plt.show()

# Reset generator
test_generator.reset()

# Predict
preds_final = model_final.predict(test_generator, verbose=0)

# Convert predictions to class labels
preds_final_classes = np.argmax(preds_final, axis=1)

# Print Confusion Matrix
cm = pd.DataFrame(
    data=confusion_matrix(test_generator.classes, preds_final_classes, labels=[0, 1, 2]),
    index=["Actual Cat", "Actual Dog", "Actual Wild"],
    columns=["Predicted Cat", "Predicted Dog", "Predicted Wild"]
)

# Plot confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Print Classification Report
print("\nClassification Report:")
print(classification_report(
    y_true=test_generator.classes,
    y_pred=preds_final_classes,
    target_names=['Cat', 'Dog', 'Wild'],
    digits=4
))

# Additional metrics per class
print("\nPer-class Accuracy:")
for i, category in enumerate(['Cat', 'Dog', 'Wild']):
    class_correct = cm.iloc[i,i]
    class_total = cm.iloc[i].sum()
    print(f"{category}: {class_correct/class_total:.4f}")

"""## Save Data"""

model_final.save('my_model.h5')

export_dir = 'saved_model/'
tf.saved_model.save(model_final, export_dir)

!pip install tensorflowjs

import os

# Membuat folder 'submission' jika belum ada
if not os.path.exists('submission'):
  os.makedirs('submission')

!pip install tensorflow_decision_forests

model_final.save('model.h5')

#Menyimpan model ke format TFLite
tflite_folder = 'submission/tflite'
tflite_model_path = f'{tflite_folder}/model.tflite'
# Pastikan folder tflite ada
import os
os.makedirs(tflite_folder, exist_ok=True)

# Mengonversi model ke format TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model_final)
tflite_model = converter.convert()

# Menyimpan model TFLite
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

# Menyimpan file label jika ada
labels = ["Label1", "Label2", "Label3"]  # Ganti dengan label yang sesuai
with open(f'{tflite_folder}/label.txt', 'w') as f:
    for label in labels:
        f.write(f"{label}\n")

#Menyimpan model ke format SavedModel
saved_model_folder = 'submission/saved_model'
model_final.save(saved_model_folder)

print(tf.__version__)

!mkdir -p submission/tfjs
!tensorflowjs_converter --input_format=keras model.h5 submission/tfjs

!pip freeze > requirements.txt

# Pindahkan ke folder submission
!mkdir -p submission
!mv requirements.txt submission/

from google.colab import files
import shutil

def zip_folder(folder_path, output_filename):
  shutil.make_archive(output_filename, 'zip', folder_path)
  files.download(output_filename + '.zip')


# Assuming you want to zip the 'submission' folder
zip_folder('submission', 'submission')



"""## Inference"""

import tensorflow as tf
import numpy as np
from PIL import Image

# Load the TensorFlow.js model
model = tf.keras.models.load_model('submission/saved_model')

# Load and preprocess the image
def preprocess_image(image_path):
  img = Image.open(image_path).convert('L')  # Load as grayscale
  img = img.resize((100, 100))  # Resize to match model input
  img_array = np.array(img) / 255.0  # Normalize pixel values
  img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
  img_array = np.expand_dims(img_array, axis=-1) # Add channel dimension
  return img_array

# Example usage
image_path = './animal_faces/cat/flickr_cat_000002.jpg'  # Replace with your image path
preprocessed_image = preprocess_image(image_path)

# Perform inference
predictions = model.predict(preprocessed_image)

# Get the predicted class
predicted_class = np.argmax(predictions)

# Print the predicted class
print(f"Predicted class: {predicted_class}")

# You can map the predicted class to the corresponding label (e.g., 0: cat, 1: dog, 2: wild)
class_labels = ['Cat', 'Dog', 'Wild']
print(f"Predicted label: {class_labels[predicted_class]}")